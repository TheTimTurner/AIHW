{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Assignment 4: Sentiment analysis with SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this programming assignment, we will revisit the problem of sensiment analysis, but using a different approach. Recall that the task is to predict the *sentiment* (positive or negative) of a single sentence taken from a review of a movie, restaurant, or product. The data set consists of 3000 labeled sentences, which we divide into a training set of size 2500 and a test set of size 500. Previously we found a logistic regression classifier. Today we will use a support vector machine.\n",
    "\n",
    "Make sure the notebook is in the same folder that contains `full_set.txt`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import string\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rc('xtick', labelsize=14) \n",
    "matplotlib.rc('ytick', labelsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set consists of 3000 sentences, each labeled '1' (if it came from a positive review) or '0' (if it came from a negative review). To be consistent with our notation from lecture, we will change the negative review label to '-1'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read in the data set.\n",
    "with open(\"full_set.txt\") as f:\n",
    "    content = f.readlines()\n",
    "    \n",
    "## Remove leading and trailing white space\n",
    "content = [x.strip() for x in content]\n",
    "\n",
    "## Separate the sentences from the labels\n",
    "sentences = [x.split(\"\\t\")[0] for x in content]\n",
    "labels = [x.split(\"\\t\")[1] for x in content]\n",
    "\n",
    "## Transform the labels from '0 v.s. 1' to '-1 v.s. 1'\n",
    "y = np.array(labels, dtype='int8')\n",
    "y = 2*y - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the text data\n",
    "\n",
    "To transform this prediction problem into one amenable to linear classification, we will first need to preprocess the text data. We will do four transformations:\n",
    "\n",
    "1. Remove punctuation and numbers.\n",
    "2. Transform all words to lower-case.\n",
    "3. Remove _stop words_.\n",
    "4. Convert the sentences into vectors, using a bag-of-words representation.\n",
    "\n",
    "We begin with first two steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "## full_remove takes a string x and a list of characters removal_list \n",
    "## returns x with all the characters in removal_list replaced by ' '\n",
    "def full_remove(x, removal_list):\n",
    "    for w in removal_list:\n",
    "        x = x.replace(w, ' ')\n",
    "    return x\n",
    "\n",
    "## Remove digits\n",
    "digits = [str(x) for x in range(10)]\n",
    "digit_less = [full_remove(x, digits) for x in sentences]\n",
    "\n",
    "## Remove punctuation\n",
    "punc_less = [full_remove(x, list(string.punctuation)) for x in digit_less]\n",
    "\n",
    "## Make everything lower-case\n",
    "sents_lower = [x.lower() for x in punc_less]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop words\n",
    "\n",
    "Stop words are words that are filtered out because they are believed to contain no useful information for the task at hand. These usually include articles such as 'a' and 'the', pronouns such as 'i' and 'they', and prepositions such 'to' and 'from'. We have put together a very small list of stop words, but these are by no means comprehensive. Feel free to use something different; for instance, larger lists can easily be found on the web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define our stop words\n",
    "stop_set = set(['the', 'a', 'an', 'i', 'he', 'she', 'they', 'to', 'of', 'it', 'from'])\n",
    "\n",
    "## Remove stop words\n",
    "sents_split = [x.split() for x in sents_lower]\n",
    "sents_processed = [\" \".join(list(filter(lambda a: a not in stop_set, x))) for x in sents_split]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of words\n",
    "\n",
    "In order to use linear classifiers on our data set, we need to transform our textual data into numeric data. The classical way to do this is known as the _bag of words_ representation. In this representation, each word is thought of as corresponding to a number in `{1, 2, ..., d}` where `d` is the size of our vocabulary. And each sentence is represented as a d-dimensional vector $x$, where $x_i$ is the number of times that word $i$ occurs in the sentence.\n",
    "\n",
    "To do this transformation, we will make use of the `CountVectorizer` class in `scikit-learn` (Note that this is the only time you can call an external function from scikit-learn). We will cap the number of features at 4500, meaning a word will make it into our vocabulary only if it is one of the 4500 most common words in the corpus. This is often a useful step as it can weed out spelling mistakes and words which occur too infrequently to be useful.\n",
    "\n",
    "Once we get the bag-of-words representation, append a '1' to the beginning of each vector to allow our linear classifier to learn a bias term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original size:  (3000, 4500)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "## Transform to bag of words representation.\n",
    "vectorizer = CountVectorizer(analyzer = \"word\", tokenizer = None, preprocessor = None, stop_words = None, max_features = 4500)\n",
    "data_features = vectorizer.fit_transform(sents_processed)\n",
    "data_mat = data_features.toarray()\n",
    "print ('The original size: ',data_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training / test split\n",
    "\n",
    "Finally, we split the data into a training set of 2500 sentences and a test set of 500 sentences (of which 250 are positive and 250 negative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data:  (2500, 4500)\n",
      "test data:  (500, 4500)\n"
     ]
    }
   ],
   "source": [
    "## Split the data into testing and training sets\n",
    "np.random.seed(0)\n",
    "test_inds = np.append(np.random.choice((np.where(y==-1))[0], 250, replace=False), np.random.choice((np.where(y==1))[0], 250, replace=False))\n",
    "train_inds = list(set(range(len(labels))) - set(test_inds))\n",
    "\n",
    "train_data = data_mat[train_inds,]\n",
    "train_labels = y[train_inds]\n",
    "\n",
    "test_data = data_mat[test_inds,]\n",
    "test_labels = y[test_inds]\n",
    "\n",
    "print(\"train data: \", train_data.shape)\n",
    "print(\"test data: \", test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Solving for soft-margin SVM\n",
    "\n",
    "\n",
    "Recall that support vector machine (SVM) finds a linear decision boundary with the largest margin for a binary classification problem. Suppose we have a training dataset $\\{(x_{1},y_1),...,(x_n,y_n)\\}$\n",
    "where $x_{i} \\in \\mathbb{R}^{d}$ are feature vectors and $y_i\\in\\{-1,+1\\}$ are labels.  The linear classifier is parametrized by $\\theta\\in \\mathbb{R}^{d}$ and $\\theta_0\\in\\mathbb{R}$, and predicts +1 at a point $x$ if $\\theta\\cdot x+\\theta_0>0$ and -1 otherwise. \n",
    "\n",
    "It turns out that the soft-margin SVM optimization is equivalent to the following unconstrained optimization:\n",
    "$$\\underset{\\theta\\in\\mathbb{R}^d,\\theta\\in\\mathbb{R}}{\\text{min}}\\|\\theta\\|^2+C\\sum_{i=1}^n\\ell_{\\mathrm{hinge}}(y_i(\\theta\\cdot x_i+\\theta_0))$$\n",
    "where  $\\ell_{\\mathrm{hinge}}(t)=\\max(0,1-t)$ is called the ``hinge loss,'' which takes value $1-t$ if $t<1$ and 0 otherwise. For example, $\\ell_{\\mathrm{hinge}}(-1)=2$, and $\\ell_{\\mathrm{hinge}}(2)=0$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that for gradient-based optimization, hinge loss may be difficult to deal with because it is not differentiable at point $t=1$. One solution is to use the ``smoothed version'' of hinge loss:\n",
    "\n",
    "$$\\ell_{\\mathrm{smooth-hinge}}(t) = \\begin{cases}\n",
    "\\frac{1}{2} - t      & \\text{if} ~~ t \\le 0, \\\\\n",
    "\\frac{1}{2} (1 - t)^2 & \\text{if} ~~ 0 < t < 1, \\\\\n",
    "0                      & \\text{if} ~~ 1 \\le t\n",
    "\\end{cases}$$\n",
    "\n",
    "\n",
    "Thus, in the rest of the problem, we will consider the following optimization:\n",
    "$$\\underset{\\theta\\in\\mathbb{R}^d,\\theta\\in\\mathbb{R}}{\\text{min}}\\|\\theta\\|^2+C\\sum_{i=1}^n\\ell_{\\mathrm{smooth-hinge}}(y_i(\\theta\\cdot x_i+\\theta_0))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task P2:** Implement the hinge loss function and the smooth hinge loss function. Plot the function $\\ell_{\\mathrm{hinge}}(t)$ and $\\ell_{\\mathrm{smooth-hinge}}(t)$for $t\\in[-5,5]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x15f9b2c2c18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x15f9a6dfcc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### STUDENT: Start of code ###\n",
    "def hinge_loss_smooth(t):\n",
    "       \n",
    "    if (t<=0):\n",
    "        return ((1/2)-t)\n",
    "    elif(t>0 and t<1):\n",
    "        return (1/2)*(pow((1-t),2))\n",
    "    else:\n",
    "        return (0)\n",
    "\n",
    "def hinge_loss(t):\n",
    "    \n",
    "    if (t<1):\n",
    "        return (1-t)\n",
    "    else :\n",
    "        return (0)\n",
    "        \n",
    "x = []\n",
    "sh = [0]*500\n",
    "h = [0]*500\n",
    "for i in range(500):\n",
    "    t = (i-250)/50\n",
    "    x.append(t)\n",
    "    sh[i]=(hinge_loss_smooth(t))\n",
    "    h[i]=(hinge_loss(t))\n",
    "    \n",
    "    \n",
    "        \n",
    "plt.plot(x,sh)\n",
    "plt.title('smooth hinge')\n",
    "plt.ylabel('hinge value')\n",
    "plt.xlabel('t value')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.plot(x,h)\n",
    "plt.title('hinge')\n",
    "plt.ylabel('hinge value')\n",
    "plt.xlabel('t value')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "### End of code ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task P5:** Let $f(\\theta,\\theta_0)= \\|\\theta\\|^2+C\\sum_{i=1}^n\\ell_{\\mathrm{smooth-hinge}}(y_i(\\theta\\cdot x_i+\\theta_0))$ be the objective function of the optimization problem we want to solve. Implement the function that obtains the partial derivative $\\frac{\\partial }{\\partial \\theta}f(\\theta,\\theta_0)$ and $\\frac{\\partial }{\\partial \\theta_0}f(\\theta,\\theta_0)$. Also, print out the output of the code that calculates the derivatives at $\\theta=1$ and $\\theta_0=1$ with $C=1$.\n",
    "\n",
    "Hint: you need to calculate the partial derivative of the smoothed hinge loss for each data point separately, and add them together to obtain the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def weight_derivative(theta, theta0, C, feature_matrix, labels):\n",
    "    # Input:\n",
    "    # theta: weight vector theta, a numpy vector of dimension d\n",
    "    # theta0: intercept theta0, a numpy vector of dimension 1\n",
    "    # C: constant C\n",
    "    # feature_matrix: numpy array of size n by d, where n is the number of data points, and d is the feature dimension\n",
    "    # labels: true labels y, a numpy vector of dimension d, each with value -1 or +1\n",
    "    # Output:\n",
    "    # Derivative of the cost function with respect to the weight theta, grad_theta\n",
    "    # Derivative of the cost function with respect to the weight theta0, grad_theta0\n",
    "    \n",
    "    \n",
    "    ## STUDENT: Start of code ###\n",
    "    score_matrix = (feature_matrix.dot(theta) + theta0)  * labels\n",
    "    \n",
    "    feature_matrix1=np.array([np.zeros(4500)]*2500)\n",
    "    feature_matrix2=np.array([np.zeros(4500)]*2500)\n",
    "    \n",
    "    for i in range(len(score_matrix)):\n",
    "        if score_matrix[i]<=0:\n",
    "            feature_matrix1[i] = feature_matrix[i]\n",
    "        elif score_matrix[i]<1:\n",
    "            feature_matrix2[i] = feature_matrix[i]\n",
    "    \n",
    "    \n",
    "    grad_theta =  2*theta-C*((feature_matrix1).T).dot(labels)-C*((feature_matrix2).T).dot(labels*(1-(feature_matrix2.dot(theta)+theta0)*labels))\n",
    "    grad_theta0 = -C*labels - C*((feature_matrix2).T).dot(labels*(1-(feature_matrix2.dot(theta)+theta0)*labels))\n",
    "    \n",
    "    \n",
    "    return grad_theta, grad_theta0\n",
    "    # End of code ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.   2.05 2.05 2.1  2.05 2.05 3.8  2.15 2.   2.  ]\n",
      "[nan nan nan ... nan nan nan]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kuent\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:28: RuntimeWarning: invalid value encountered in sqrt\n"
     ]
    }
   ],
   "source": [
    "# STUDENT: PRINT THE OUTPUT AND COPY IT TO THE SOLUTION FILE\n",
    "theta = np.ones(data_mat.shape[1]) # a weight of all 1s\n",
    "\n",
    "theta0 = np.ones(1) # a number 1\n",
    "C = 0.05\n",
    "grad_theta, grad_theta0 = weight_derivative(theta, theta0, C,train_data,train_labels)\n",
    "\n",
    "print (grad_theta[:10])\n",
    "print (grad_theta0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task P6:**  For sentiment analysis data, choose a value for the trade-off parameter $C$. Report the training error at convergence and the testing error. \n",
    "\n",
    "Note:  you can just use the same gradient descent algorithm that we wrote in assignment 2, or use the adam_optimizer provided below.\n",
    "\n",
    "Here is an [article](https://towardsdatascience.com/adam-latest-trends-in-deep-learning-optimization-6be9a291375c#:~:text=Adam%20%5B1%5D%20is%20an%20adaptive,for%20training%20deep%20neural%20networks.&text=The%20algorithms%20leverages%20the,learning%20rates%20for%20each%20parameter) on the Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(feature_matrix, labels, theta,theta0, C):\n",
    "    score = (feature_matrix.dot(theta)+theta0)*labels\n",
    "    return np.sum(theta**2)+C*np.sum([hinge_loss_smooth(t) for t in score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adam_optimizer(feature_matrix, labels, initial_theta,initial_theta0, C, step_size=0.01, tolerence=0.01, b1=0.9, b2=0.999, eps=10**-8):\n",
    "    # Gradient descent algorithm for logistic regression problem    \n",
    "    \n",
    "    # Input:\n",
    "    # feature_matrix: numpy array of size n by d, where n is the number of data points, and d is the feature dimension\n",
    "    # labels: true labels y, a numpy vector of dimension d\n",
    "    # initial_theta: initial theta to start with, a numpy vector of dimension d\n",
    "    # initial_theta0: initial theta0 to start with, a numpy vector of dimension 1\n",
    "    # step_size: step size of update\n",
    "    # tolerance: tolerace epsilon for stopping condition\n",
    "    # Parameters by Adam optimizer\n",
    "    # Output:\n",
    "    # Weights obtained after convergence\n",
    "\n",
    "    converged = False \n",
    "    m = np.zeros(len(initial_theta))\n",
    "    v = np.zeros(len(initial_theta))\n",
    "    m0 = np.zeros(1)\n",
    "    v0 = np.zeros(1)\n",
    "    theta = np.array(initial_theta) # current iterate\n",
    "    theta0 = np.array(initial_theta0) # current iterate\n",
    "    i = 0\n",
    "    while not converged:\n",
    "        # impelementation of what the gradient descent algorithm does in every iteration\n",
    "        # Refer back to the update rule listed above: update the weight\n",
    "        i += 1\n",
    "        grad_theta, grad_theta0  = weight_derivative(theta, theta0, C,feature_matrix, labels)\n",
    "        \n",
    "        m = (1 - b1) * grad_theta      + b1 * m  # First  moment estimate.\n",
    "        v = (1 - b2) * (grad_theta**2) + b2 * v  # Second moment estimate.\n",
    "        mhat = m / (1 - b1**(i + 1))    # Bias correction.\n",
    "        vhat = v / (1 - b2**(i + 1))\n",
    "        theta = theta - step_size*mhat/(np.sqrt(vhat) + eps)\n",
    "        \n",
    "        m0 = (1 - b1) * grad_theta0      + b1 * m0  # First  moment estimate.\n",
    "        v0 = (1 - b2) * (grad_theta0**2) + b2 * v0  # Second moment estimate.\n",
    "        mhat0 = m0 / (1 - b1**(i + 1))    # Bias correction.\n",
    "        vhat0 = v0 / (1 - b2**(i + 1))\n",
    "        theta0 = theta0 - step_size*mhat0/(np.sqrt(vhat0) + eps)\n",
    "        \n",
    "        # Compute the gradient magnitude:\n",
    "        \n",
    "        gradient_magnitude = np.sqrt(np.sum(grad_theta**2))\n",
    "        \n",
    "        # Check the stopping condition to decide whether you want to stop the iterations\n",
    "        \n",
    "        if gradient_magnitude < tolerance:\n",
    "            converged = True\n",
    "        \n",
    "        preds_train = model_predict(train_data,theta,theta0)\n",
    "\n",
    "        ## Compute errors\n",
    "        errs_train = np.sum((preds_train > 0.0) != (train_labels > 0.0))\n",
    "\n",
    "        print (\"Iteration: \",i,\"objective: \",objective(feature_matrix, labels, theta,theta0, C),\"tr err: \",float(errs_train)/len(train_labels),\"gradient_magnitude: \", gradient_magnitude) # for us to check about convergence\n",
    "        \n",
    "    return(theta, theta0)\n",
    "\n",
    "### End of code ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_predict(feature_matrix,theta,theta0):\n",
    "# Prediction made by SVM \n",
    "    \n",
    "    # Input:\n",
    "    # feature_matrix: numpy array of size n by d+1, where n is the number of data points, and d+1 is the feature dimension\n",
    "    #                 note we have included the dummy feature as the first column of the feature_matrix\n",
    "    # theta: weight theta, a numpy vector of dimension d\n",
    "    # theta0: weight theta0, a numpy vector of dimension 1\n",
    "    # Output:\n",
    "    # labels: predicted labels, a numpy vector of dimension n\n",
    "    \n",
    "    h =  feature_matrix.dot(theta)+theta0\n",
    "    y_h = (h >= 0)*2-1\n",
    "    \n",
    "    return y_h    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  1 objective:  6132.002384737767 tr err:  0.5 gradient_magnitude:  881.9631511576887\n",
      "Iteration:  2 objective:  5901.298958462611 tr err:  0.4928 gradient_magnitude:  851.9168536152582\n",
      "Iteration:  3 objective:  8153.635347348822 tr err:  0.4156 gradient_magnitude:  874.2477459175442\n",
      "Iteration:  4 objective:  6070.666612871248 tr err:  0.3052 gradient_magnitude:  872.160573355283\n",
      "Iteration:  5 objective:  3942.8606332887957 tr err:  0.2136 gradient_magnitude:  642.6425885406783\n",
      "Iteration:  6 objective:  4497.444488751299 tr err:  0.282 gradient_magnitude:  164.40509587437742\n",
      "Iteration:  7 objective:  6083.321631328058 tr err:  0.364 gradient_magnitude:  325.3494131170822\n",
      "Iteration:  8 objective:  7130.767917665976 tr err:  0.4036 gradient_magnitude:  532.2129220342773\n",
      "Iteration:  9 objective:  7000.960018360936 tr err:  0.3888 gradient_magnitude:  610.8794208436875\n",
      "Iteration:  10 objective:  6241.668476222676 tr err:  0.3396 gradient_magnitude:  571.0675011093479\n",
      "Iteration:  11 objective:  5954.151158555325 tr err:  0.308 gradient_magnitude:  405.84488532404475\n",
      "Iteration:  12 objective:  6562.892402992044 tr err:  0.3112 gradient_magnitude:  181.52196274378022\n",
      "Iteration:  13 objective:  7533.982583178618 tr err:  0.3216 gradient_magnitude:  116.58842799744573\n",
      "Iteration:  14 objective:  8175.115649264161 tr err:  0.3352 gradient_magnitude:  227.48231688202014\n",
      "Iteration:  15 objective:  8312.116873190316 tr err:  0.3364 gradient_magnitude:  283.66536805716515\n",
      "Iteration:  16 objective:  8367.461112757133 tr err:  0.3452 gradient_magnitude:  238.6469133925518\n",
      "Iteration:  17 objective:  8810.18576488125 tr err:  0.3528 gradient_magnitude:  104.76755351473776\n",
      "Iteration:  18 objective:  9570.80713678517 tr err:  0.3656 gradient_magnitude:  103.04303967003725\n",
      "Iteration:  19 objective:  10363.460170691655 tr err:  0.3752 gradient_magnitude:  202.89997847481345\n",
      "Iteration:  20 objective:  10947.643468409531 tr err:  0.3844 gradient_magnitude:  264.48009481451095\n",
      "Iteration:  21 objective:  11292.067936615536 tr err:  0.3828 gradient_magnitude:  289.5247187573308\n",
      "Iteration:  22 objective:  11649.446618127276 tr err:  0.378 gradient_magnitude:  228.65844714092682\n",
      "Iteration:  23 objective:  12223.76087183676 tr err:  0.384 gradient_magnitude:  131.34245125809494\n",
      "Iteration:  24 objective:  12935.149558927964 tr err:  0.3888 gradient_magnitude:  63.647676451050415\n",
      "[-0.13397748  0.23881118 -0.52610802 ... -1.79137435 -0.07706427\n",
      " -0.22903722]\n",
      "[24.43469044]\n"
     ]
    }
   ],
   "source": [
    "# Initialize the weights, step size and tolerance\n",
    "# Start of code\n",
    "initial_theta = np.ones(data_mat.shape[1]) ## STUDENT: initialize theta\n",
    "initial_theta0 = 1   ## STUDENT: initialize theta0\n",
    "C =  1## STUDENT: choose the C\n",
    "step_size =  1## STUDENT: choose the step_size\n",
    "tolerance =  100## STUDENT: choose the tolerance\n",
    "\n",
    "# end of code\n",
    "\n",
    "theta, theta0 = adam_optimizer(train_data,train_labels, initial_theta, initial_theta0,C, step_size, tolerance)\n",
    "print (theta)\n",
    "print (theta0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24.43469044]\n",
      "Training error:  0.3888\n",
      "Test error:  0.414\n",
      "Training error:  0.3888\n",
      "Test error:  0.414\n"
     ]
    }
   ],
   "source": [
    "# STUDENT: copy the output of this section to the solution file\n",
    "\n",
    "## Get predictions on training and test data\n",
    "preds_train = model_predict(train_data,theta,theta0)\n",
    "preds_test = model_predict(test_data,theta,theta0)\n",
    "##\n",
    "##compute errors not working properly\n",
    "##\n",
    "def getError(preds_train,train_labels):\n",
    "    errs_train =  0\n",
    "    for i in range(len(preds_train)):\n",
    "        if((preds_train[i] > 0.0) and (train_labels[i] < 0.0)):\n",
    "            errs_train+=1\n",
    "        if((preds_train[i] < 0.0) and (train_labels[i] > 0.0)):\n",
    "            errs_train+=1\n",
    "            \n",
    "            \n",
    "    return errs_train\n",
    "\n",
    "## Compute errors\n",
    "\n",
    "errs_train = np.sum((preds_train > 0.0) != (train_labels > 0.0))\n",
    "errs_test = np.sum((preds_test > 0.0) != (test_labels > 0.0))\n",
    "print(theta0)\n",
    "\n",
    "error_train = getError(preds_train,train_labels)\n",
    "error_test = getError(preds_test,test_labels)\n",
    "\n",
    "print (\"Training error: \", float(errs_train)/len(train_labels))\n",
    "print (\"Test error: \", float(errs_test)/len(test_labels))\n",
    "\n",
    "print (\"Training error: \", float(error_train)/len(train_labels))\n",
    "print (\"Test error: \", float(error_test)/len(test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task P7:**  List 4 example sentences that are correctly classified by SVM, and 4 example sentences that are  incorrectly classified by SVM. Explain what you have found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STUDENT: your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
