{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Assignment 3: Sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this programming assignment, you will implement logistic regression to predict the sentiment of reviews that come from `imdb.com`, `amazon.com`, and `yelp.com`. Make sure the notebook is in the same folder that contains `full_set.txt`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import string\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rc('xtick', labelsize=14) \n",
    "matplotlib.rc('ytick', labelsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set consists of 3000 sentences, each labeled '1' (if it came from a positive review) or '0' (if it came from a negative review). To be consistent with our notation from lecture, we will change the negative review label to '-1'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label:  -1 ;  So there is no way for me to plug it in here in the US unless I go by a converter.\n",
      "Label:  1 ;  Good case, Excellent value.\n",
      "Label:  1 ;  Great for the jawbone.\n",
      "Label:  -1 ;  Tied to charger for conversations lasting more than 45 minutes.MAJOR PROBLEMS!!\n",
      "Label:  1 ;  The mic is great.\n"
     ]
    }
   ],
   "source": [
    "## Read in the data set.\n",
    "with open(\"full_set.txt\") as f:\n",
    "    content = f.readlines()\n",
    "    \n",
    "## Remove leading and trailing white space\n",
    "content = [x.strip() for x in content]\n",
    "\n",
    "## Separate the sentences from the labels\n",
    "sentences = [x.split(\"\\t\")[0] for x in content]\n",
    "labels = [x.split(\"\\t\")[1] for x in content]\n",
    "\n",
    "## Transform the labels from '0 v.s. 1' to '-1 v.s. 1'\n",
    "y = np.array(labels, dtype='int8')\n",
    "y = 2*y - 1\n",
    "\n",
    "for i in range(5):\n",
    "    print ('Label: ',y[i],'; ', sentences[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the text data\n",
    "\n",
    "To transform this prediction problem into one amenable to linear classification, we will first need to preprocess the text data. We will do four transformations:\n",
    "\n",
    "1. Remove punctuation and numbers.\n",
    "2. Transform all words to lower-case.\n",
    "3. Remove _stop words_.\n",
    "4. Convert the sentences into vectors, using a bag-of-words representation.\n",
    "\n",
    "We begin with first two steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {},
   "outputs": [],
   "source": [
    "## full_remove takes a string x and a list of characters removal_list \n",
    "## returns x with all the characters in removal_list replaced by ' '\n",
    "def full_remove(x, removal_list):\n",
    "    for w in removal_list:\n",
    "        x = x.replace(w, ' ')\n",
    "    return x\n",
    "\n",
    "## Remove digits\n",
    "digits = [str(x) for x in range(10)]\n",
    "digit_less = [full_remove(x, digits) for x in sentences]\n",
    "\n",
    "## Remove punctuation\n",
    "punc_less = [full_remove(x, list(string.punctuation)) for x in digit_less]\n",
    "\n",
    "## Make everything lower-case\n",
    "sents_lower = [x.lower() for x in punc_less]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop words\n",
    "\n",
    "Stop words are words that are filtered out because they are believed to contain no useful information for the task at hand. These usually include articles such as 'a' and 'the', pronouns such as 'i' and 'they', and prepositions such 'to' and 'from'. We have put together a very small list of stop words, but these are by no means comprehensive. Feel free to use something different; for instance, larger lists can easily be found on the web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define our stop words\n",
    "stop_set = set(['the', 'a', 'an', 'i', 'he', 'she', 'they', 'to', 'of', 'it', 'from'])\n",
    "\n",
    "## Remove stop words\n",
    "sents_split = [x.split() for x in sents_lower]\n",
    "sents_processed = [\" \".join(list(filter(lambda a: a not in stop_set, x))) for x in sents_split]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do the sentences look like so far?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['so there is no way for me plug in here in us unless go by converter',\n",
       " 'good case excellent value',\n",
       " 'great for jawbone',\n",
       " 'tied charger for conversations lasting more than minutes major problems',\n",
       " 'mic is great',\n",
       " 'have jiggle plug get line up right get decent volume',\n",
       " 'if you have several dozen or several hundred contacts then imagine fun sending each them one by one',\n",
       " 'if you are razr owner you must have this',\n",
       " 'needless say wasted my money',\n",
       " 'what waste money and time']"
      ]
     },
     "execution_count": 618,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents_processed[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of words\n",
    "\n",
    "In order to use linear classifiers on our data set, we need to transform our textual data into numeric data. The classical way to do this is known as the _bag of words_ representation. In this representation, each word is thought of as corresponding to a number in `{1, 2, ..., d}` where `d` is the size of our vocabulary. And each sentence is represented as a d-dimensional vector $x$, where $x_i$ is the number of times that word $i$ occurs in the sentence.\n",
    "\n",
    "To do this transformation, we will make use of the `CountVectorizer` class in `scikit-learn` (Note that this is the only time you can call an external function from scikit-learn). We will cap the number of features at 4500, meaning a word will make it into our vocabulary only if it is one of the 4500 most common words in the corpus. This is often a useful step as it can weed out spelling mistakes and words which occur too infrequently to be useful.\n",
    "\n",
    "**Task P1:** Once you get the bag-of-words representation, append a '1' to the beginning of each vector to allow our linear classifier to learn a bias term. What is the size of the resulting data_mat matrix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original size:  (3000, 4500)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "## Transform to bag of words representation.\n",
    "vectorizer = CountVectorizer(analyzer = \"word\", tokenizer = None, preprocessor = None, stop_words = None, max_features = 4500)\n",
    "data_features = vectorizer.fit_transform(sents_processed)\n",
    "\n",
    "print ('The original size: ',data_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The updated size:  (3000, 4501)\n"
     ]
    }
   ],
   "source": [
    "## STUDENT: YOUR CODE STARTS HERE\n",
    "# Task: Append '1' to the beginning of each vector.\n",
    "# Hint: You can use data_features.toarray() to transform data_features into a numpy array\n",
    "# The output should be a numpy array named data_mat\n",
    "\n",
    "data_mat = data_features.toarray()\n",
    "\n",
    "data_mat=np.insert(data_mat,0,1.,axis =1)\n",
    "\n",
    "## STUDENT: CODE ENDS\n",
    "print ('The updated size: ',data_mat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training / test split\n",
    "\n",
    "Finally, we split the data into a training set of 2500 sentences and a test set of 500 sentences (of which 250 are positive and 250 negative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data:  (2500, 4501)\n",
      "test data:  (500, 4501)\n"
     ]
    }
   ],
   "source": [
    "## Split the data into testing and training sets\n",
    "np.random.seed(0)\n",
    "test_inds = np.append(np.random.choice((np.where(y==-1))[0], 250, replace=False), np.random.choice((np.where(y==1))[0], 250, replace=False))\n",
    "train_inds = list(set(range(len(labels))) - set(test_inds))\n",
    "\n",
    "train_data = data_mat[train_inds,]\n",
    "train_labels = y[train_inds]\n",
    "\n",
    "test_data = data_mat[test_inds,]\n",
    "test_labels = y[test_inds]\n",
    "\n",
    "print(\"train data: \", train_data.shape)\n",
    "print(\"test data: \", test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Fitting a logistic regression model to the training data\n",
    "\n",
    "In this section, we will implement our own logistic regression solver using gradient descent. As we have seen in the class, to learn the parameters of logistic regression, we need to perform the following optimization:\n",
    "\n",
    "$$\n",
    "{\\bf \\tilde\\theta}_t =\n",
    "\\underset{{\\bf \\tilde\\theta}}{\\operatorname{argmin}} \\; L_\\mathcal{D}({\\bf \\tilde\\theta}) =\n",
    "\\underset{{\\bf \\tilde\\theta}}{\\operatorname{argmin}} \\;\\sum_{i=1}^{n} \\ln \\left( 1 + e^{y_i \\; {\\bf \\tilde\\theta}_t^T {\\tilde x}_i} \\right)\n",
    "$$\n",
    "where $y_i\\in\\{-1,+1\\}$ is the label, ${\\bf \\tilde\\theta}$ is the vector of coefficients:\n",
    "$$\n",
    "{\\bf \\tilde\\theta} = \\begin{bmatrix} \\theta_0 & \\theta_1 & ... & \\theta_d \\end{bmatrix}^T,\n",
    "$$\n",
    "and ${\\tilde x}$ is the \"augmented\" feature vector (of $d+1$ dimensions), where we stick a 1 in the front of the original features:\n",
    "$$\n",
    "{\\tilde x} = \\begin{bmatrix} 1 & x_1 & ... & x_d \\end{bmatrix}^T.\n",
    "$$\n",
    "\n",
    "\n",
    "There is no nice, closed-form solution like with [least-squares linear regression](http://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_pseudoinverse) so we will use [gradient descent](http://en.wikipedia.org/wiki/Gradient_descent) instead. Specifically we will use batch gradient descent which calculates the gradient from all data points in the data set. Luckily, the loss function $L_\\mathcal{D}({\\bf \\tilde\\theta})$ we want to minimize is [convex](http://en.wikipedia.org/wiki/Convex_optimization) so there is only one minimum. Thus the minimum we arrive at is the global minimum.\n",
    "\n",
    "Gradient descent is a general method and requires twice differentiability for [smoothness](http://en.wikipedia.org/wiki/Smooth_function). It updates the parameters using a first-order approximation of the error surface.\n",
    "\n",
    "$$\n",
    "{\\bf \\tilde\\theta}_{t+1} = {\\bf \\tilde\\theta}_t + \\nabla L_\\mathcal{D}({\\bf \\tilde\\theta}_t)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task P2:** Derive the gradient of the loss $L_\\mathcal{D}({\\bf \\tilde\\theta})$ with respect to ${\\bf \\tilde\\theta}$, namely $\\nabla L_\\mathcal{D}({\\bf \\tilde\\theta}_t)$. The answer should depend on data points $(x_i,y_i)$ for $i=1,...,n$, and the model parameter ${\\bf \\tilde\\theta}$. Make sure you get the sign correct. Also implement the function `weight_derivative`. Print the output of the code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_derivative(weights, feature_matrix, labels):\n",
    "    # Input:\n",
    "    # weights: weight vector w, a numpy vector of dimension d\n",
    "    # feature_matrix: numpy array of size n by d, where n is the number of data points, and d is the feature dimension\n",
    "    # labels: true labels y, a numpy vector of dimension d, each with value -1 or +1\n",
    "    # Output:\n",
    "    # Derivative of the regression cost function with respect to the weight w, a numpy array of dimension d\n",
    "        \n",
    "    ## STUDENT: Start of code ###\n",
    "    \n",
    "    #    - yx\n",
    "    #  ____________\n",
    "    #   ((e^yθ^Tx)+1)\n",
    "    \n",
    "    #initialize ld(theta)\n",
    "    derivatives=np.zeros(weights.size)\n",
    "\n",
    "    for i in range(labels.size):\n",
    "        #dot product of weigths with each column of feature matrix\n",
    "        column_product=np.dot(weights,feature_matrix[i,:])\n",
    "        temp=labels[i]*column_product\n",
    "        value = 1/(1+np.exp(temp))\n",
    "        #take the sum for every road\n",
    "        derivatives = derivatives+(- labels[i]*feature_matrix[i,:]*value)\n",
    "        \n",
    "    return derivatives  \n",
    "\n",
    "    # End of code ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.23415330e+03 -4.13993755e-08  1.00000000e+00  9.99993856e-01\n",
      "  1.99987630e+00  9.99859072e-01  9.52574127e-01  3.59772270e+01\n",
      "  2.99996572e+00 -1.38879439e-11]\n"
     ]
    }
   ],
   "source": [
    "# STUDENT: PRINT THE OUTPUT AND COPY IT TO THE SOLUTION FILE\n",
    "my_weights = np.ones(data_mat.shape[1]) # a weight of all 1s\n",
    "derivative = weight_derivative(my_weights,train_data,train_labels)\n",
    "\n",
    "\n",
    "print (derivative[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can just use the same gradient descent algorithm that we wrote in assignment 2 to solve it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(feature_matrix, labels, initial_weights, step_size, tolerance):\n",
    "    # Gradient descent algorithm for logistic regression problem    \n",
    "    \n",
    "    # Input:\n",
    "    # feature_matrix: numpy array of size n by d, where n is the number of data points, and d is the feature dimension\n",
    "    # labels: true labels y, a numpy vector of dimension d\n",
    "    # initial_weights: initial weight vector to start with, a numpy vector of dimension d\n",
    "    # step_size: step size of update\n",
    "    # tolerance: tolerace epsilon for stopping condition\n",
    "    # Output:\n",
    "    # Weights obtained after convergence\n",
    "\n",
    "    converged = False \n",
    "    weights = np.array(initial_weights) # current iterate\n",
    "    i = 0\n",
    "    \n",
    "    while not converged:\n",
    "        # impelementation of what the gradient descent algorithm does in every iteration\n",
    "        # Refer back to the update rule listed above: update the weight\n",
    "        i += 1\n",
    "        derivative = np.array(weight_derivative(weights, feature_matrix, labels))\n",
    "        #print(step_size*derivative)\n",
    "        weights -= (step_size * derivative)\n",
    "       \n",
    "        # Compute the gradient magnitude:\n",
    "        \n",
    "        gradient_magnitude = np.sqrt(np.sum(derivative**2))\n",
    "        \n",
    "        # Check the stopping condition to decide whether you want to stop the iterations\n",
    "        \n",
    "       \n",
    "        if (gradient_magnitude < tolerance):\n",
    "            converged = True\n",
    "        \n",
    "        print (\"Iteration: \",i,\"gradient_magnitude: \", gradient_magnitude) # for us to check about convergence\n",
    "        \n",
    "    return(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task P3:** Specify the initial_weights, step_size, and tolerance for the function `gradient_descent`. Copy the outputs of the code to the solution file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  1 gradient_magnitude:  1417.6129100613616\n",
      "Iteration:  2 gradient_magnitude:  1573.259355605426\n",
      "Iteration:  3 gradient_magnitude:  1291.3793043542978\n",
      "Iteration:  4 gradient_magnitude:  1573.259355605426\n",
      "Iteration:  5 gradient_magnitude:  1493.4879431356328\n",
      "Iteration:  6 gradient_magnitude:  1572.127857395829\n",
      "Iteration:  7 gradient_magnitude:  1389.4120202171252\n",
      "Iteration:  8 gradient_magnitude:  1547.9211801082454\n",
      "Iteration:  9 gradient_magnitude:  1400.8397808294674\n",
      "Iteration:  10 gradient_magnitude:  1423.7696432810585\n",
      "Iteration:  11 gradient_magnitude:  1333.317358974452\n",
      "Iteration:  12 gradient_magnitude:  1277.138306824494\n",
      "Iteration:  13 gradient_magnitude:  1244.2778248794955\n",
      "Iteration:  14 gradient_magnitude:  1210.1461014971637\n",
      "Iteration:  15 gradient_magnitude:  1205.058972957471\n",
      "Iteration:  16 gradient_magnitude:  1193.212271347353\n",
      "Iteration:  17 gradient_magnitude:  1182.244631804018\n",
      "Iteration:  18 gradient_magnitude:  1179.1263641832315\n",
      "Iteration:  19 gradient_magnitude:  1164.23450865317\n",
      "Iteration:  20 gradient_magnitude:  1148.2402603279368\n",
      "Iteration:  21 gradient_magnitude:  1134.5036487224243\n",
      "Iteration:  22 gradient_magnitude:  1122.3129420699265\n",
      "Iteration:  23 gradient_magnitude:  1107.560029484869\n",
      "Iteration:  24 gradient_magnitude:  1108.466588640464\n",
      "Iteration:  25 gradient_magnitude:  1102.2835594492976\n",
      "Iteration:  26 gradient_magnitude:  1098.1995009531943\n",
      "Iteration:  27 gradient_magnitude:  1081.391123914706\n",
      "Iteration:  28 gradient_magnitude:  1061.616577138208\n",
      "Iteration:  29 gradient_magnitude:  1024.7786512625403\n",
      "Iteration:  30 gradient_magnitude:  1000.0812457721107\n",
      "Iteration:  31 gradient_magnitude:  1005.3590111714384\n",
      "Iteration:  32 gradient_magnitude:  1017.4813020007085\n",
      "Iteration:  33 gradient_magnitude:  1031.834504260735\n",
      "Iteration:  34 gradient_magnitude:  1045.4402429615557\n",
      "Iteration:  35 gradient_magnitude:  1039.339427939935\n",
      "Iteration:  36 gradient_magnitude:  1000.0084004065419\n",
      "Iteration:  37 gradient_magnitude:  941.952217298465\n",
      "Iteration:  38 gradient_magnitude:  878.548291692355\n",
      "Iteration:  39 gradient_magnitude:  842.1369005575219\n",
      "Iteration:  40 gradient_magnitude:  833.6105552452442\n",
      "Iteration:  41 gradient_magnitude:  844.3870494501903\n",
      "Iteration:  42 gradient_magnitude:  871.1314888813798\n",
      "Iteration:  43 gradient_magnitude:  893.9312947758663\n",
      "Iteration:  44 gradient_magnitude:  927.7718421224708\n",
      "Iteration:  45 gradient_magnitude:  963.1790531119536\n",
      "Iteration:  46 gradient_magnitude:  998.2098898762359\n",
      "Iteration:  47 gradient_magnitude:  1017.4275282451521\n",
      "Iteration:  48 gradient_magnitude:  1008.3764708533232\n",
      "Iteration:  49 gradient_magnitude:  974.9289245882045\n",
      "Iteration:  50 gradient_magnitude:  878.4533946610622\n",
      "Iteration:  51 gradient_magnitude:  754.6217086427482\n",
      "Iteration:  52 gradient_magnitude:  671.0600957811374\n",
      "Iteration:  53 gradient_magnitude:  610.2089285582649\n",
      "Iteration:  54 gradient_magnitude:  571.8755588076551\n",
      "Iteration:  55 gradient_magnitude:  541.6356836513617\n",
      "Iteration:  56 gradient_magnitude:  528.1689225261583\n",
      "Iteration:  57 gradient_magnitude:  530.6614661152408\n",
      "Iteration:  58 gradient_magnitude:  541.4246566057933\n",
      "Iteration:  59 gradient_magnitude:  574.4705255885729\n",
      "Iteration:  60 gradient_magnitude:  616.7483474127854\n",
      "Iteration:  61 gradient_magnitude:  684.5009040203792\n",
      "Iteration:  62 gradient_magnitude:  802.3269414834019\n",
      "Iteration:  63 gradient_magnitude:  925.0335733645647\n",
      "Iteration:  64 gradient_magnitude:  1034.2569875578597\n",
      "Iteration:  65 gradient_magnitude:  1126.5185926339657\n",
      "Iteration:  66 gradient_magnitude:  1155.1690875740203\n",
      "Iteration:  67 gradient_magnitude:  1112.880299428954\n",
      "Iteration:  68 gradient_magnitude:  1019.3136196711904\n",
      "Iteration:  69 gradient_magnitude:  881.2222108699194\n",
      "Iteration:  70 gradient_magnitude:  701.886271744936\n",
      "Iteration:  71 gradient_magnitude:  518.0041256932822\n",
      "Iteration:  72 gradient_magnitude:  394.2158324099065\n",
      "Iteration:  73 gradient_magnitude:  254.2085463728818\n",
      "Iteration:  74 gradient_magnitude:  184.6554319714206\n",
      "Iteration:  75 gradient_magnitude:  120.91244695979078\n",
      "Iteration:  76 gradient_magnitude:  90.5063602042317\n",
      "Iteration:  77 gradient_magnitude:  75.36940686608037\n",
      "Iteration:  78 gradient_magnitude:  65.78660319592403\n",
      "Iteration:  79 gradient_magnitude:  57.83763881138234\n",
      "Iteration:  80 gradient_magnitude:  54.13589653558388\n",
      "Iteration:  81 gradient_magnitude:  59.30929676422846\n",
      "Iteration:  82 gradient_magnitude:  75.3820627140077\n",
      "Iteration:  83 gradient_magnitude:  100.69653988021996\n",
      "Iteration:  84 gradient_magnitude:  119.90899497132757\n",
      "Iteration:  85 gradient_magnitude:  156.00202112599717\n",
      "Iteration:  86 gradient_magnitude:  177.85877466018067\n",
      "Iteration:  87 gradient_magnitude:  205.67400518125513\n",
      "Iteration:  88 gradient_magnitude:  227.36642911944344\n",
      "Iteration:  89 gradient_magnitude:  249.18122575250203\n",
      "Iteration:  90 gradient_magnitude:  273.5628362996189\n",
      "Iteration:  91 gradient_magnitude:  309.87372211511894\n",
      "Iteration:  92 gradient_magnitude:  344.43816353012227\n",
      "Iteration:  93 gradient_magnitude:  397.73460203036814\n",
      "Iteration:  94 gradient_magnitude:  466.82482271139764\n",
      "Iteration:  95 gradient_magnitude:  564.8005267820674\n",
      "Iteration:  96 gradient_magnitude:  680.6093709516746\n",
      "Iteration:  97 gradient_magnitude:  832.6381122592827\n",
      "Iteration:  98 gradient_magnitude:  1037.568677675099\n",
      "Iteration:  99 gradient_magnitude:  1213.9896592171424\n",
      "Iteration:  100 gradient_magnitude:  1265.9214360183498\n",
      "Iteration:  101 gradient_magnitude:  1195.8635349331955\n",
      "Iteration:  102 gradient_magnitude:  1071.9328396263013\n",
      "Iteration:  103 gradient_magnitude:  961.3513103290152\n",
      "Iteration:  104 gradient_magnitude:  805.7602429616513\n",
      "Iteration:  105 gradient_magnitude:  627.4624727093312\n",
      "Iteration:  106 gradient_magnitude:  457.6945343053209\n",
      "Iteration:  107 gradient_magnitude:  272.9448326307267\n",
      "Iteration:  108 gradient_magnitude:  185.60497350688138\n",
      "Iteration:  109 gradient_magnitude:  87.152766946668\n",
      "Iteration:  110 gradient_magnitude:  59.61917123295565\n",
      "Iteration:  111 gradient_magnitude:  45.27291070920764\n",
      "Iteration:  112 gradient_magnitude:  40.58273027491615\n",
      "Iteration:  113 gradient_magnitude:  37.72263618284012\n",
      "Iteration:  114 gradient_magnitude:  35.49587694170745\n",
      "Iteration:  115 gradient_magnitude:  38.05040018902745\n",
      "Iteration:  116 gradient_magnitude:  44.789747594694695\n",
      "Iteration:  117 gradient_magnitude:  53.81390898974061\n",
      "Iteration:  118 gradient_magnitude:  57.067060681016216\n",
      "Iteration:  119 gradient_magnitude:  57.326820935526555\n",
      "Iteration:  120 gradient_magnitude:  58.04539307660754\n",
      "Iteration:  121 gradient_magnitude:  57.71939128860156\n",
      "Iteration:  122 gradient_magnitude:  52.32743316883459\n",
      "Iteration:  123 gradient_magnitude:  40.3639842607014\n",
      "Iteration:  124 gradient_magnitude:  29.209209864813882\n",
      "Iteration:  125 gradient_magnitude:  22.757091689807744\n",
      "Iteration:  126 gradient_magnitude:  21.104358622170473\n",
      "Iteration:  127 gradient_magnitude:  21.403520104225535\n",
      "Iteration:  128 gradient_magnitude:  26.176362973792948\n",
      "Iteration:  129 gradient_magnitude:  47.737898076451145\n",
      "Iteration:  130 gradient_magnitude:  71.08544351267692\n",
      "Iteration:  131 gradient_magnitude:  88.31080923089208\n",
      "Iteration:  132 gradient_magnitude:  107.06430651493396\n",
      "Iteration:  133 gradient_magnitude:  130.35174669289077\n",
      "Iteration:  134 gradient_magnitude:  141.7653313910372\n",
      "Iteration:  135 gradient_magnitude:  150.97499546419564\n",
      "Iteration:  136 gradient_magnitude:  159.21402950969437\n",
      "Iteration:  137 gradient_magnitude:  161.48305793569833\n",
      "Iteration:  138 gradient_magnitude:  149.58178034547942\n",
      "Iteration:  139 gradient_magnitude:  120.27042889055953\n",
      "Iteration:  140 gradient_magnitude:  82.2027887935232\n",
      "Iteration:  141 gradient_magnitude:  47.38994908204069\n",
      "Iteration:  142 gradient_magnitude:  39.975404616456906\n",
      "Iteration:  143 gradient_magnitude:  35.97398589813837\n",
      "Iteration:  144 gradient_magnitude:  30.075260414364585\n",
      "Iteration:  145 gradient_magnitude:  25.260892864788037\n",
      "Iteration:  146 gradient_magnitude:  23.623314165449806\n",
      "Iteration:  147 gradient_magnitude:  20.285095749460687\n",
      "Iteration:  148 gradient_magnitude:  19.3167425496746\n",
      "Iteration:  149 gradient_magnitude:  30.029075069380944\n",
      "Iteration:  150 gradient_magnitude:  58.072262866795064\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  151 gradient_magnitude:  87.59044063961097\n",
      "Iteration:  152 gradient_magnitude:  93.14357549653658\n",
      "Iteration:  153 gradient_magnitude:  101.98874999358657\n",
      "Iteration:  154 gradient_magnitude:  107.79357858989196\n",
      "Iteration:  155 gradient_magnitude:  113.53852789009858\n",
      "Iteration:  156 gradient_magnitude:  110.84071508990469\n",
      "Iteration:  157 gradient_magnitude:  85.69028468501882\n",
      "Iteration:  158 gradient_magnitude:  63.146889126469986\n",
      "Iteration:  159 gradient_magnitude:  35.680474017389564\n",
      "Iteration:  160 gradient_magnitude:  19.62838101643654\n",
      "Iteration:  161 gradient_magnitude:  15.044064056951033\n",
      "Iteration:  162 gradient_magnitude:  14.881977192533368\n",
      "Iteration:  163 gradient_magnitude:  16.884799818787943\n",
      "Iteration:  164 gradient_magnitude:  20.99612830813034\n",
      "Iteration:  165 gradient_magnitude:  28.46616335552794\n",
      "Iteration:  166 gradient_magnitude:  38.54231613360385\n",
      "Iteration:  167 gradient_magnitude:  49.53342011092665\n",
      "Iteration:  168 gradient_magnitude:  65.89399629922268\n",
      "Iteration:  169 gradient_magnitude:  86.63201068333733\n",
      "Iteration:  170 gradient_magnitude:  97.32900134388298\n",
      "Iteration:  171 gradient_magnitude:  119.6736751088901\n",
      "Iteration:  172 gradient_magnitude:  142.692434671309\n",
      "Iteration:  173 gradient_magnitude:  164.45871717344136\n",
      "Iteration:  174 gradient_magnitude:  188.82511277935788\n",
      "Iteration:  175 gradient_magnitude:  223.96703119379063\n",
      "Iteration:  176 gradient_magnitude:  260.65297985103336\n",
      "Iteration:  177 gradient_magnitude:  313.7855158630328\n",
      "Iteration:  178 gradient_magnitude:  367.4048663768909\n",
      "Iteration:  179 gradient_magnitude:  438.5482040128968\n",
      "Iteration:  180 gradient_magnitude:  513.3108156414276\n",
      "Iteration:  181 gradient_magnitude:  640.7874761497172\n",
      "Iteration:  182 gradient_magnitude:  823.8940296471002\n",
      "Iteration:  183 gradient_magnitude:  1033.545047908645\n",
      "Iteration:  184 gradient_magnitude:  1239.788922031654\n",
      "Iteration:  185 gradient_magnitude:  1297.9454322658917\n",
      "Iteration:  186 gradient_magnitude:  1253.7275181097846\n",
      "Iteration:  187 gradient_magnitude:  1161.6460706196988\n",
      "Iteration:  188 gradient_magnitude:  985.4234171618904\n",
      "Iteration:  189 gradient_magnitude:  805.3352735404562\n",
      "Iteration:  190 gradient_magnitude:  592.6776008580945\n",
      "Iteration:  191 gradient_magnitude:  371.7479706799473\n",
      "Iteration:  192 gradient_magnitude:  230.12223174216405\n",
      "Iteration:  193 gradient_magnitude:  74.5193639158971\n",
      "Iteration:  194 gradient_magnitude:  44.81854145868827\n",
      "Iteration:  195 gradient_magnitude:  33.914001854981144\n",
      "Iteration:  196 gradient_magnitude:  29.520539217469928\n",
      "Iteration:  197 gradient_magnitude:  27.477251976184917\n",
      "Iteration:  198 gradient_magnitude:  27.183300067789325\n",
      "Iteration:  199 gradient_magnitude:  23.86031175888893\n",
      "Iteration:  200 gradient_magnitude:  22.223261969940662\n",
      "Iteration:  201 gradient_magnitude:  23.46867284891726\n",
      "Iteration:  202 gradient_magnitude:  26.515009211607506\n",
      "Iteration:  203 gradient_magnitude:  32.93769468921445\n",
      "Iteration:  204 gradient_magnitude:  38.8376344705613\n",
      "Iteration:  205 gradient_magnitude:  40.43561437869443\n",
      "Iteration:  206 gradient_magnitude:  31.774364026576325\n",
      "Iteration:  207 gradient_magnitude:  22.073731536979633\n",
      "Iteration:  208 gradient_magnitude:  15.99410728427953\n",
      "Iteration:  209 gradient_magnitude:  12.280681357690243\n",
      "Iteration:  210 gradient_magnitude:  12.988611986812371\n",
      "Iteration:  211 gradient_magnitude:  13.428321199031883\n",
      "Iteration:  212 gradient_magnitude:  15.293753517508899\n",
      "Iteration:  213 gradient_magnitude:  20.601047917992947\n",
      "Iteration:  214 gradient_magnitude:  25.732355707005095\n",
      "Iteration:  215 gradient_magnitude:  35.65019020697128\n",
      "Iteration:  216 gradient_magnitude:  39.42217472131075\n",
      "Iteration:  217 gradient_magnitude:  41.73859695683973\n",
      "Iteration:  218 gradient_magnitude:  43.8850717877552\n",
      "Iteration:  219 gradient_magnitude:  40.632868234638394\n",
      "Iteration:  220 gradient_magnitude:  26.122161413993428\n",
      "Iteration:  221 gradient_magnitude:  9.599949468415904\n",
      "Iteration:  222 gradient_magnitude:  8.463966824199485\n",
      "Iteration:  223 gradient_magnitude:  8.2077416660145\n",
      "Iteration:  224 gradient_magnitude:  7.7580632414366235\n",
      "Iteration:  225 gradient_magnitude:  11.302451609885987\n",
      "Iteration:  226 gradient_magnitude:  14.559194918788295\n",
      "Iteration:  227 gradient_magnitude:  15.590028966493128\n",
      "Iteration:  228 gradient_magnitude:  13.699106692924763\n",
      "Iteration:  229 gradient_magnitude:  11.195419492412165\n",
      "Iteration:  230 gradient_magnitude:  7.521759515840023\n",
      "Iteration:  231 gradient_magnitude:  6.488055494774333\n",
      "Iteration:  232 gradient_magnitude:  6.483020492426743\n",
      "Iteration:  233 gradient_magnitude:  8.555573263369112\n",
      "Iteration:  234 gradient_magnitude:  10.115181051444209\n",
      "Iteration:  235 gradient_magnitude:  14.506882429698313\n",
      "Iteration:  236 gradient_magnitude:  22.028876573498966\n",
      "Iteration:  237 gradient_magnitude:  25.75299765962638\n",
      "Iteration:  238 gradient_magnitude:  27.566018533075322\n",
      "Iteration:  239 gradient_magnitude:  28.763962533996192\n",
      "Iteration:  240 gradient_magnitude:  24.765018935582404\n",
      "Iteration:  241 gradient_magnitude:  12.719996212682497\n",
      "Iteration:  242 gradient_magnitude:  6.154789510628705\n",
      "Iteration:  243 gradient_magnitude:  5.752933974301095\n",
      "Iteration:  244 gradient_magnitude:  5.999886579047668\n",
      "Iteration:  245 gradient_magnitude:  6.75316641946886\n",
      "Iteration:  246 gradient_magnitude:  7.76602604566065\n",
      "Iteration:  247 gradient_magnitude:  8.796183337039182\n",
      "Iteration:  248 gradient_magnitude:  11.746967231120166\n",
      "Iteration:  249 gradient_magnitude:  11.195105406687107\n",
      "Iteration:  250 gradient_magnitude:  10.572989988740899\n",
      "Iteration:  251 gradient_magnitude:  8.12804846824885\n",
      "Iteration:  252 gradient_magnitude:  6.054059774882603\n",
      "Iteration:  253 gradient_magnitude:  6.103143897972643\n",
      "Iteration:  254 gradient_magnitude:  12.513724350297002\n",
      "Iteration:  255 gradient_magnitude:  21.151343719937095\n",
      "Iteration:  256 gradient_magnitude:  33.70504648826548\n",
      "Iteration:  257 gradient_magnitude:  46.58886619827601\n",
      "Iteration:  258 gradient_magnitude:  57.178713862570895\n",
      "Iteration:  259 gradient_magnitude:  68.99652263625494\n",
      "Iteration:  260 gradient_magnitude:  73.12604087354478\n",
      "Iteration:  261 gradient_magnitude:  68.52164245403362\n",
      "Iteration:  262 gradient_magnitude:  52.71794775566094\n",
      "Iteration:  263 gradient_magnitude:  20.72130007463245\n",
      "Iteration:  264 gradient_magnitude:  6.309426429781083\n",
      "Iteration:  265 gradient_magnitude:  5.85529020663293\n",
      "Iteration:  266 gradient_magnitude:  6.015969054224693\n",
      "Iteration:  267 gradient_magnitude:  6.373523214122213\n",
      "Iteration:  268 gradient_magnitude:  6.07644307514241\n",
      "Iteration:  269 gradient_magnitude:  6.322646961484458\n",
      "Iteration:  270 gradient_magnitude:  7.828958939074733\n",
      "Iteration:  271 gradient_magnitude:  7.963993596917696\n",
      "Iteration:  272 gradient_magnitude:  8.87795653791727\n",
      "Iteration:  273 gradient_magnitude:  8.57475562911773\n",
      "Iteration:  274 gradient_magnitude:  7.063224604492798\n",
      "Iteration:  275 gradient_magnitude:  6.302499985192718\n",
      "Iteration:  276 gradient_magnitude:  6.313235282702932\n",
      "Iteration:  277 gradient_magnitude:  6.06427420829406\n",
      "Iteration:  278 gradient_magnitude:  9.18598886267549\n",
      "Iteration:  279 gradient_magnitude:  12.394962818390496\n",
      "Iteration:  280 gradient_magnitude:  17.383722570607656\n",
      "Iteration:  281 gradient_magnitude:  20.863712009165074\n",
      "Iteration:  282 gradient_magnitude:  21.17741359040548\n",
      "Iteration:  283 gradient_magnitude:  17.092006566923803\n",
      "Iteration:  284 gradient_magnitude:  6.625432330454185\n",
      "Iteration:  285 gradient_magnitude:  4.7084077903887405\n",
      "Iteration:  286 gradient_magnitude:  4.773088172138191\n",
      "Iteration:  287 gradient_magnitude:  4.724700660594436\n",
      "Iteration:  288 gradient_magnitude:  6.710432745764432\n",
      "Iteration:  289 gradient_magnitude:  6.218729358906206\n",
      "Iteration:  290 gradient_magnitude:  4.028534754378524\n",
      "Iteration:  291 gradient_magnitude:  3.1903767127002607\n",
      "Iteration:  292 gradient_magnitude:  3.026292174303805\n",
      "Iteration:  293 gradient_magnitude:  3.080815726428183\n",
      "Iteration:  294 gradient_magnitude:  3.793536173138131\n",
      "Iteration:  295 gradient_magnitude:  4.427320105734775\n",
      "Iteration:  296 gradient_magnitude:  7.323837161532204\n",
      "Iteration:  297 gradient_magnitude:  7.256358075469979\n",
      "Iteration:  298 gradient_magnitude:  8.612787444273907\n",
      "Iteration:  299 gradient_magnitude:  8.93795822711556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  300 gradient_magnitude:  5.776558030925981\n",
      "Iteration:  301 gradient_magnitude:  3.4454576386039815\n",
      "Iteration:  302 gradient_magnitude:  2.8891066710720694\n",
      "Iteration:  303 gradient_magnitude:  2.8142806000520424\n",
      "Iteration:  304 gradient_magnitude:  2.7745649641453243\n",
      "Iteration:  305 gradient_magnitude:  2.9011307721678574\n",
      "Iteration:  306 gradient_magnitude:  4.282728345857269\n",
      "Iteration:  307 gradient_magnitude:  8.915377546753835\n",
      "Iteration:  308 gradient_magnitude:  9.978338344005941\n",
      "Iteration:  309 gradient_magnitude:  12.375388776550794\n",
      "Iteration:  310 gradient_magnitude:  14.374213361365427\n",
      "Iteration:  311 gradient_magnitude:  14.889701429733611\n",
      "Iteration:  312 gradient_magnitude:  11.391844876941724\n",
      "Iteration:  313 gradient_magnitude:  4.205711373895891\n",
      "Iteration:  314 gradient_magnitude:  2.8736410651141124\n",
      "Iteration:  315 gradient_magnitude:  2.819016442841977\n",
      "Iteration:  316 gradient_magnitude:  2.7819374339541643\n",
      "Iteration:  317 gradient_magnitude:  2.705978008565047\n",
      "Iteration:  318 gradient_magnitude:  2.185158718002818\n",
      "Iteration:  319 gradient_magnitude:  2.980339246378145\n",
      "Iteration:  320 gradient_magnitude:  4.655067045338705\n",
      "Iteration:  321 gradient_magnitude:  4.791498272926847\n",
      "Iteration:  322 gradient_magnitude:  3.1931727895408217\n",
      "Iteration:  323 gradient_magnitude:  2.7630533402950337\n",
      "Iteration:  324 gradient_magnitude:  2.93293394005811\n",
      "Iteration:  325 gradient_magnitude:  3.2755898756591026\n",
      "Iteration:  326 gradient_magnitude:  4.558368979452793\n",
      "Iteration:  327 gradient_magnitude:  5.575125485887412\n",
      "Iteration:  328 gradient_magnitude:  4.762117841702198\n",
      "Iteration:  329 gradient_magnitude:  3.507773346135192\n",
      "Iteration:  330 gradient_magnitude:  2.661051740251205\n",
      "Iteration:  331 gradient_magnitude:  2.1594020532725176\n",
      "Iteration:  332 gradient_magnitude:  2.382037546798874\n",
      "Iteration:  333 gradient_magnitude:  3.7458383489823617\n",
      "Iteration:  334 gradient_magnitude:  10.736443650062032\n",
      "Iteration:  335 gradient_magnitude:  14.942005419304614\n",
      "Iteration:  336 gradient_magnitude:  18.932180232379\n",
      "Iteration:  337 gradient_magnitude:  19.924477473839126\n",
      "Iteration:  338 gradient_magnitude:  18.84271784945446\n",
      "Iteration:  339 gradient_magnitude:  9.273232948902919\n",
      "Iteration:  340 gradient_magnitude:  2.3692676805497253\n",
      "Iteration:  341 gradient_magnitude:  2.0338454392852543\n",
      "Iteration:  342 gradient_magnitude:  1.9613701258390488\n",
      "Here are the final weights after convergence:\n",
      "[  -0.61437196   28.50040686  -59.49999998 ... -335.3688858     0.5\n",
      "  -61.49358349]\n"
     ]
    }
   ],
   "source": [
    "#Initialize the weights, step size and tolerance\n",
    "# Start of code\n",
    "#STUDENT: Specify the initial_weights, step_size, and tolerance\n",
    "\n",
    "temp_weight = [0.5]*(len(train_data[0]))\n",
    "initial_weights = np.array(temp_weight)\n",
    "#print(initial_weights)\n",
    "\n",
    "\n",
    "step_size = 2\n",
    "tolerance = 2\n",
    "# end of code\n",
    "\n",
    "# Use the regression_gradient_descent function to calculate the gradient decent and store it in the variable 'final_weights'\n",
    "final_weights = gradient_descent(train_data,train_labels, initial_weights, step_size, tolerance)\n",
    "\n",
    "# end of code\n",
    "print (\"Here are the final weights after convergence:\")\n",
    "print (final_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task P4:** Write the code to extract the y-intercept $\\theta_0$ and the rest of the parameters $\\theta= \\begin{bmatrix} \\theta_1 & ... & \\theta_d \\end{bmatrix}^T$. Copy the code and print the outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y intercept:  5.301077114064977\n",
      "theta1 and theta2:  -152.5 -317.50000000000006\n"
     ]
    }
   ],
   "source": [
    "## STUDENT: CODE STARTS HERE\n",
    "## Pull out the parameters (theta_0, theta) of the logistic regression model\n",
    "\n",
    "theta =  final_weights #gradient_descent(train_data,train_labels, initial_weights, step_size, tolerance)\n",
    "theta0 = theta[0]\n",
    "theta = np.delete(theta,0)\n",
    "\n",
    "## STUDENT: CODE ENDS HERE\n",
    "\n",
    "print ('y intercept: ',theta0)\n",
    "print ('theta1 and theta2: ',theta[1],theta[2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the [logistic/sigmoid function](http://en.wikipedia.org/wiki/Logistic_function) is given by\n",
    "\n",
    "$$\n",
    "f(z)=\\frac{1}{1+e^{-z}}\n",
    "$$\n",
    "\n",
    "Based on the logistic model, the posterior probability\n",
    "\n",
    "$$\n",
    "P(y\\mid{\\bf x})=f(y\\; {\\bf \\tilde \\theta}^T {\\bf \\tilde x})\n",
    "$$\n",
    "\n",
    "To make a prediction, we can simply choose the label $y\\in\\{-1,+1\\}$ with the higher posterior probability.\n",
    "\n",
    "**Task P5:** Write the code to make the prediction for a given data matrix. Report the training error and test error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_predict(feature_matrix,weights):\n",
    "# Prediction made by logistic regression   \n",
    "    \n",
    "    # Input:\n",
    "    # feature_matrix: numpy array of size n by d+1, where n is the number of data points, and d+1 is the feature dimension\n",
    "    #                 note we have included the dummy feature as the first column of the feature_matrix\n",
    "    # weights: weight vector to start with, a numpy vector of dimension d+1\n",
    "    # Output:\n",
    "    # labels: predicted labels, a numpy vector of dimension n\n",
    "    \n",
    "    ## STUDENT: YOUR CODE HERE\n",
    "    predict_array = []\n",
    "   \n",
    "    for i in range(len(feature_matrix)):\n",
    "       \n",
    "        dot = np.dot(feature_matrix[i],weights)\n",
    "        \n",
    "        z = -1*dot\n",
    "        predict = 1/(1+pow(np.e,z))\n",
    "        predict_array.append(predict)\n",
    "    return np.array(predict_array)\n",
    "    ## STUDENT: CODE ENDS\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error:  0.0004\n",
      "Test error:  0.002\n",
      "Training error:  0.1028\n",
      "Test error:  0.184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kuent\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:19: RuntimeWarning: overflow encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "# STUDENT: copy the output of this section to the solution file\n",
    "def getError(preds_train,train_labels):\n",
    "    errs_train =  0\n",
    "    for i in range(len(preds_train)):\n",
    "        if((preds_train[i] > 0.0) and (train_labels[i] < 0.0)):\n",
    "            errs_train+=1\n",
    "        if((preds_train[i] < 0.0) and (train_labels[i] > 0.0)):\n",
    "            errs_train+=1\n",
    "    return errs_train\n",
    "    \n",
    "## Get predictions on training and test data\n",
    "preds_train = model_predict(train_data,final_weights)\n",
    "#print(preds_train)\n",
    "preds_test = model_predict(test_data,final_weights)\n",
    "\n",
    "\n",
    "## Compute errors\n",
    "\n",
    "errs_train = np.sum((preds_train > 0.0) is not (train_labels > 0.0))\n",
    "errs_test = np.sum((preds_test > 0.0) is not (test_labels > 0.0))\n",
    "\n",
    "error_train = getError(preds_train,train_labels)\n",
    "error_test = getError(preds_test,test_labels)\n",
    "    \n",
    "    \n",
    "\n",
    "print (\"Training error: \", float(errs_train)/len(train_labels))\n",
    "print (\"Test error: \", float(errs_test)/len(test_labels))\n",
    "\n",
    "print (\"Training error: \", float(error_train)/len(train_labels))\n",
    "print (\"Test error: \", float(error_test)/len(test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Analyzing the margin\n",
    "\n",
    "As discussed in the lecture, the logistic regression model produces not just classifications but also conditional probability estimates. \n",
    "\n",
    "We will say that `x` has **margin** `gamma` if (according to the logistic regression model) `Pr(y=1|x) > (1/2)+gamma` or `Pr(y=1|x) < (1/2)-gamma`. For example, if `Pr(y=1|x)` is 0.7 according to the logistic regression model, then the margin is 0.2. If `Pr(y=1|x)` is 0.15 according to the logistic regression model, then the margin is 0.35.\n",
    "\n",
    "**Task P6:** Implement the following function **margin_counts** that takes as input the learned weights $\\bf\\tilde\\theta$, the feature matrix (`feature_matrix`), and a value of `gamma`, and computes how many points in the data have margin at least `gamma`. Copy the code and the output plot (i.e., visualization of the test set's distribution of margin values) to the solution file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {},
   "outputs": [],
   "source": [
    "def margin_counts(feature_matrix, weights, gamma):\n",
    "## Return number of points for which Pr(y=1) lies in [0, 0.5 - gamma) or (0.5 + gamma, 1]\n",
    "\n",
    "    # Input:\n",
    "    # feature_matrix: numpy array of size n by d+1, where n is the number of data points, and d+1 is the feature dimension\n",
    "    #                 note we have included the dummy feature as the first column of the feature_matrix\n",
    "    # weights: weight vector to start with, a numpy vector of dimension d+1\n",
    "    # gamma: the margin value\n",
    "    # Output:\n",
    "    # number of points for which Pr(y=1) lies in [0, 0.5 - gamma) or (0.5 + gamma, 1]\n",
    "    \n",
    "    ## STUDENT: YOUR CODE HERE\n",
    "    model_predict_arr = model_predict(feature_matrix, weights)\n",
    "    count =0;\n",
    "    for i in range(len(model_predict_arr)):\n",
    "        if (model_predict_arr[i]>0) and (model_predict_arr[i]<1):\n",
    "            if(model_predict_arr[i]<0.5-gamma) or (model_predict_arr[i]>0.5+gamma):\n",
    "                count+=1\n",
    "    \n",
    "    return count\n",
    "\n",
    "    ## STUDENT: CODE ENDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now visualize the test set's distribution of margin values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kuent\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:19: RuntimeWarning: overflow encountered in double_scalars\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2117b4a6ac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gammas = np.arange(0,0.5,0.01)\n",
    "f = np.vectorize(lambda g: margin_counts(test_data, final_weights,g))\n",
    "plt.plot(gammas, f(gammas)/500.0, linewidth=2, color='green')\n",
    "plt.xlabel('Margin', fontsize=14)\n",
    "plt.ylabel('Fraction of points above margin', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we investigate a natural question: Are points `x` with larger margin more likely to be classified correctly?\n",
    "\n",
    "To address this, we define a function **margin_errors** that computes the fraction of points with margin at least `gamma` that are misclassified.\n",
    "\n",
    "**Task P7:** Implement the function `margin_errors` that computes the fraction of points with margin at least `gamma` that are misclassified. Copy the code and the output plot (i.e., visualization of the relationship between margin and error rate) to the solution file. What do you observe from the plot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {},
   "outputs": [],
   "source": [
    "def margin_errors(feature_matrix, labels, weights, gamma):\n",
    "## Return error of predictions that lie in intervals [0, 0.5 - gamma) and (0.5 + gamma, 1]\n",
    "\n",
    "    # Input:\n",
    "    # feature_matrix: numpy array of size n by d+1, where n is the number of data points, and d+1 is the feature dimension\n",
    "    #                 note we have included the dummy feature as the first column of the feature_matrix\n",
    "    # labels: true labels y, a numpy vector of dimension n\n",
    "    # weights: weight vector to start with, a numpy vector of dimension d+1\n",
    "    # gamma: the margin value\n",
    "    # Output:\n",
    "    # error of predictions that lie in intervals [0, 0.5 - gamma) and (0.5 + gamma, 1]\n",
    "    \n",
    "    ## STUDENT: YOUR CODE HERE\n",
    "    model_predict_arr = model_predict(feature_matrix, weights)\n",
    "    denominator=len(feature_matrix)\n",
    "    numerator = 1\n",
    "    \n",
    "    for i in range(len(model_predict_arr)):\n",
    "        if (model_predict_arr[i]>0) and (model_predict_arr[i]<1):\n",
    "            if(model_predict_arr[i]<(0.5-gamma)) or (model_predict_arr[i]>(0.5+gamma)):\n",
    "                denominator+=1\n",
    "                if(labels[i]<(0.5-gamma)) and (model_predict_arr[i]<(0.5-gamma)):\n",
    "                    numerator+=1\n",
    "                if(labels[i]>(0.5+gamma)) and (model_predict_arr[i]>(0.5+gamma)):\n",
    "                    numerator+=1\n",
    "              \n",
    "            \n",
    "   \n",
    "    return numerator/denominator\n",
    "\n",
    "    ## STUDENT: YOUR CODE ENDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now visualize the relationship between margin and error rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kuent\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:19: RuntimeWarning: overflow encountered in double_scalars\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2117b28aa20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Create grid of gamma values\n",
    "gammas = np.arange(0, 0.5, 0.01)\n",
    "\n",
    "## Compute margin_errors on test data for each value of g\n",
    "f = np.vectorize(lambda g: margin_errors(test_data, test_labels,final_weights, g))\n",
    "\n",
    "## Plot the result\n",
    "plt.plot(gammas, f(gammas), linewidth=2)\n",
    "plt.ylabel('Error rate', fontsize=14)\n",
    "plt.xlabel('Margin', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Words with large influence\n",
    "\n",
    "Finally, we attempt to partially **interpret** the logistic regression model.\n",
    "\n",
    "Which words are most important in deciding whether a sentence is positive? As a first approximation to this, we simply take the words whose coefficients in $\\theta$ have the largest positive values.\n",
    "\n",
    "Likewise, we look at the words whose coefficients in $\\theta$ have the most negative values, and we think of these as influential in negative predictions.\n",
    "\n",
    "**Task P8:** Report the top 10 positive words (i.e., words with the largest positive coefficients of $\\theta$) and the top 10 negative words (i.e., words with the most negative coefficients of $\\theta$). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4500\n",
      "Top Ten\n",
      "[['you', 522.3124682700212], ['well', 1198.4122353355472], ['family', 1267.4999997273922], ['happy', 1507.5298332164937], ['friendly', 1609.6197732734986], ['amazing', 1756.0169505164492], ['works', 1953.3096254243846], ['fantastic', 2054.3429738609047], ['loved', 2349.88442359567], ['great', 2382.2088423393316]]\n",
      "Bottom Ten\n",
      "[['worst', -2435.1562271465036], ['disappointment', -2236.3075883660986], ['waste', -2118.928162274578], ['disappointing', -1820.0104102997275], ['avoid', -1655.739111789458], ['bland', -1609.8454208770834], ['fails', -1431.4384892762353], ['aren', -1322.6989221304673], ['difficult', -1264.999999999803], ['writing', -824.9671458584996]]\n"
     ]
    }
   ],
   "source": [
    "## Convert vocabulary into a list:\n",
    "## This is a list where the i-th entry corresponds to the \n",
    "\n",
    "vocab = np.array([z[0] for z in sorted(vectorizer.vocabulary_.items(), key=lambda x:x[1])])\n",
    "\n",
    "\n",
    "## STUDENT: YOUR CODE HERE\n",
    "#the first index is the larges value and the last index is the smallest value\n",
    "#the value that is going to be excluded firrst from the array is in the back\n",
    "\n",
    "weights= np.delete(final_weights,0)\n",
    "print(len(weights))\n",
    "large_positive = []\n",
    "small_negative = []\n",
    "                  \n",
    "for i in range(len(weights)):\n",
    "    #initialize array\n",
    "    if (len(large_positive)<10):\n",
    "        large_positive.append([vocab[i],weights[i]])\n",
    "        small_negative.append([vocab[i],weights[i]])\n",
    "    else:\n",
    "        #larger than smallest value in array\n",
    "        if (final_weights[i]>large_positive[0][1]):\n",
    "            #remove small add large\n",
    "            large_positive.pop(0)\n",
    "            large_positive.append([vocab[i],weights[i]])\n",
    "            large_positive.sort(key=lambda x: x[1])\n",
    "        #smaller than largest value in array\n",
    "        if (final_weights[i]<small_negative[9][1]):\n",
    "            #remove large add small\n",
    "            small_negative.pop(9)\n",
    "            small_negative.append([vocab[i],weights[i]])\n",
    "            small_negative.sort(key=lambda x: x[1])\n",
    "\n",
    "#remove weights from print statement\n",
    "final_positive = []\n",
    "final_negative = []\n",
    "for i in range(10):\n",
    "    final_positive.append(large_positive[i][0])\n",
    "    final_negative.append(small_negative[i][0])\n",
    "    \n",
    "\n",
    "print('Top Ten')\n",
    "print(large_positive)\n",
    "print('Bottom Ten')\n",
    "print(small_negative)\n",
    "\n",
    "## STUDENT: CODE ENDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. (Bonus question) Classifiers that can abstain\n",
    "\n",
    "Suppose you are building a classifier, and can tolerate an error rate of at most some value `e`. Unfortunately, every classifier you try has a higher error than this. \n",
    "\n",
    "Therefore, you decide that the classifier is allowed to occasionally **abstain**: that is, to say *\"don't know\"*. When it actually makes a prediction, it must have error rate at most `e`. And subject to this constraint, it should abstain as infrequently as possible.\n",
    "\n",
    "How would you build an abstaining classifier of this kind, starting from a logistic regression model? To get the bonus score, you need to show the following:\n",
    "\n",
    "* A general description of the method\n",
    "* Your code implementation\n",
    "* A case study to show how you can use it in practice (including necessary plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {},
   "outputs": [],
   "source": [
    "## STUDENT: YOUR CODE HERE\n",
    "\n",
    "\n",
    "## STUDENT: CODE ENDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4.0,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
